### **Metrics와 Logs의 차이**

- **Metrics**: 특정 시간 동안 측정된 숫자의 시리즈로, 시스템 성능과 상태를 정량적으로 파악하는 데 사용됨.
- **Logs**: 시스템에서 발생하는 이벤트를 탐색하고 분석하는 데 사용됨. 주로 문제 원인을 조사하는 데 유용함.

**사용 예**: 애플리케이션 성능 저하 문제가 발생했을 때,

1. **Metrics**를 통해 높은 지연(latency)을 감지하고,
2. **Logs**를 분석하여 애플리케이션 오류를 조사함.

---
### **모니터링 기법**

1. **Black-box 모니터링**
    - 애플리케이션 외부에서 시스템을 관찰하는 전통적인 방법임.
    - CPU, 메모리, 스토리지 등 인프라 수준의 모니터링에 유용함.
    - 예: 클러스터 상태 점검을 위해 pod를 스케줄링하여 성공 여부로 구성 요소의 건강 상태를 확인함.
2. **White-box 모니터링**
    - 애플리케이션 상태의 세부적인 컨텍스트를 분석함.
    - HTTP 요청 수, 500 오류 발생 횟수, 요청 지연 시간 등 심층적인 데이터를 수집하여 문제의 원인을 파악함.
    - 예: "디스크가 왜 가득 찼는가?"와 같은 질문에 답할 수 있음.

---
### **모니터링 패턴**

모니터링은 전통적으로 사용되던 방식에서 발전하여, **Kubernetes와 같은 동적이고 일시적인 환경**에 적합한 새로운 접근 방식을 요구함. 다음은 분산 시스템 모니터링에 유용한 두 가지 주요 패턴임.

### **1. USE 방법론**

**창시자:** Brendan Gregg  
**목적:** **인프라 모니터링**에 중점.  
**핵심 요소:**

- **U—Utilization (이용률):** 자원이 얼마나 사용되고 있는가?
- **S—Saturation (포화):** 자원이 얼마나 한계에 가까운가?
- **E—Errors (오류):** 오류율은 어떠한가?

**적용 예:**  
클러스터 노드의 네트워크 상태를 점검하기 위해 다음을 모니터링:

- 네트워크 **이용률**
- 네트워크 **포화 상태**
- 네트워크 **오류율**

**특징:**  
자원 제약과 오류를 빠르게 식별하는 데 유용하지만, 애플리케이션 수준의 모니터링에는 한계가 있음.

### **2. RED 방법론**

**창시자:** Tom Willke  
**목적:** **애플리케이션 및 사용자 경험 모니터링**에 중점.  
**핵심 요소:**

- **R—Rate (속도):** 요청 처리 속도
- **E—Errors (오류):** 실패한 요청의 비율
- **D—Duration (지속 시간):** 요청 처리 시간

**Google의 Four Golden Signals**에서 영감을 받음:

1. **Latency (지연):** 요청을 처리하는 데 걸리는 시간
2. **Traffic (트래픽):** 시스템에 가해지는 요청량
3. **Errors (오류):** 실패한 요청 비율
4. **Saturation (포화):** 서비스 자원 활용도

**적용 예:**  
Kubernetes에서 프론트엔드 서비스를 모니터링:

- 프론트엔드 서비스가 처리하는 요청 수
- 사용자에게 반환되는 500 오류 수
- 서비스가 과부하 상태인지 여부

### **USE와 RED 방법론의 관계**

- **USE:** 인프라 수준의 자원 상태 모니터링.
- **RED:** 사용자 경험 및 애플리케이션 성능 모니터링.

이 두 방법론은 상호 보완적이며, 인프라와 애플리케이션 전반에 걸친 효율적인 모니터링을 가능하게 함.

--- 

### **Kubernetes Metrics 개요 요약**

Kubernetes 클러스터의 모니터링을 위해 **컨트롤 플레인 구성요소**와 **워크 노드 구성요소**를 모두 모니터링해야 함.

#### **클러스터 구성요소**
1. **컨트롤 플레인**: API Server, etcd, Scheduler, Controller Manager  
2. **워크 노드**: Kubelet, Container Runtime, Kube-Proxy, Kube-DNS, Pods  

이를 위해 Kubernetes는 다양한 방식으로 메트릭스를 노출하며, 주요 도구는 다음과 같음.

---

### **1. cAdvisor**
- **역할**: 컨테이너의 CPU, 메모리, 디스크 I/O 메트릭 수집.  
- **특징**: 
  - Kubernetes의 kubelet에 내장되어 모든 노드에서 실행.
  - Linux 커널의 cgroup과 statfs를 통해 리소스 사용 데이터를 수집.
- **주요 포인트**: 컨테이너 메트릭스의 **정확한 정보 소스**로 간주됨.

---

### **2. Metrics Server**
- **역할**: CPU 및 메모리와 같은 리소스 메트릭을 수집하고 HPA/VPA 및 스케줄러에 사용.
- **특징**:
  - 이전의 Heapster를 대체하는 표준 리소스 메트릭 API 제공.
  - kubelet API로부터 메트릭 수집 후 메모리에 저장.
  - **Custom Metrics API**를 통해 사용 사례에 맞는 사용자 정의 메트릭 지원.
    - 예: Prometheus의 Custom Metrics Adapter를 통해 HPA를 큐 크기와 같은 외부 메트릭 기반으로 확장 가능.

---

### **3. kube-state-metrics**
- **역할**: Kubernetes 객체 상태를 추적하는 추가 기능.  
- **특징**:
  - 리소스 사용량에 대한 정보를 제공하는 cAdvisor, Metrics Server와는 달리, Kubernetes 객체의 **상태를 모니터링**.
  - 지원하는 객체 유형은 현재 22개 이상.
- **예시 질문**:
  - **Pods**: 배포된 Pod 수, Pending 상태 Pod 수, 리소스 부족 여부.
  - **Deployments**: 실행 중인 Pod 수와 원하는 상태의 Pod 수 비교, 업데이트된 배포 확인.
  - **Nodes**: 노드 상태, 클러스터의 할당 가능한 CPU 코어 수, 스케줄링 불가 노드 존재 여부.
  - **Jobs**: 작업 시작/완료 시간, 실패한 작업 수.

**Github 문서**에서 지속적으로 확장 중인 객체 목록과 자세한 정보를 확인할 수 있음.

---

### **Kubernetes에서 모니터링해야 할 메트릭**

모든 것을 모니터링하는 것이 이상적으로 보일 수 있지만, 너무 많은 데이터를 수집하면 중요한 신호를 가리는 **노이즈**가 발생할 수 있음. **계층화된 접근 방식**을 사용하면 주요 문제를 더 효과적으로 파악할 수 있음.

#### **계층화된 모니터링 방식**
1. **물리적 또는 가상 노드**  
2. **클러스터 구성 요소**  
3. **클러스터 애드온**  
4. **최종 사용자 애플리케이션**  

이런 계층적인 접근은 문제를 더욱 체계적으로 해결할 수 있게 해줌. 예를 들어, **Pod가 Pending 상태**로 들어갈 경우, 먼저 노드의 리소스 사용량을 확인하고, 문제가 없으면 클러스터 구성 요소를 조사함.

### **모니터링해야 할 주요 메트릭**
1. **노드**  
   - CPU 사용량  
   - 메모리 사용량  
   - 네트워크 사용량  
   - 디스크 사용량  

2. **클러스터 구성 요소**  
   - etcd 지연 시간 (latency)

3. **클러스터 애드온**  
   - 클러스터 Autoscaler 성능  
   - Ingress 컨트롤러 성능  

4. **애플리케이션**  
   - 컨테이너 메모리 사용량 및 포화도  
   - 컨테이너 CPU 사용량  
   - 컨테이너 네트워크 사용량 및 오류율  
   - 프레임워크나 애플리케이션 특화 메트릭

---

### **Kubernetes와 통합 가능한 인기 모니터링 도구**

#### **1. Prometheus**
- **종류**: 오픈 소스 모니터링 및 알림 툴킷.  
- **주요 기능**:
  - Kubernetes와 네이티브 통합.  
  - 강력한 커뮤니티 지원.  
  - 시간 기반 데이터 수집 및 커스텀 메트릭 지원.  
- **운영 주체**: 2016년 CNCF(클라우드 네이티브 컴퓨팅 재단) 프로젝트로 채택.

#### **2. InfluxDB**
- **종류**: 시간 기반 데이터베이스.  
- **주요 기능**:
  - 높은 쓰기/쿼리 부하 처리 가능.  
  - TICK 스택(Telegraf, InfluxDB, Chronograf, Kapacitor)의 일부.  
  - IoT 및 DevOps 메트릭 관리에 적합.

#### **3. Datadog**
- **종류**: SaaS 기반 데이터 분석 플랫폼.  
- **주요 기능**:
  - 서버, 데이터베이스, 도구 및 서비스를 모니터링.  
  - 클라우드 네이티브 환경에서 확장 가능한 모니터링 제공.

#### **4. Sysdig**
- **종류**: 상용 모니터링 도구.  
- **주요 기능**:
  - Docker 및 Kubernetes에 최적화된 모니터링.  
  - Prometheus 메트릭 수집 및 Kubernetes 네이티브 통합 지원.

#### **5. 클라우드 제공사 도구**
   - **Google Cloud Stackdriver**  
     - GKE(Google Kubernetes Engine) 전용 설계.  
     - 모니터링과 로깅 통합.  
     - GKE 전용 대시보드 제공.
   - **Azure Monitor for Containers**  
     - Azure Kubernetes Service(AKS) 지원.  
     - Kubernetes Metrics API를 통해 CPU, 메모리 메트릭 수집 및 로그 관리.
   - **AWS Container Insights**  
     - ECS, EKS, 또는 EC2에서 실행되는 Kubernetes 지원.  
     - CPU, 메모리, 디스크, 네트워크와 같은 리소스 메트릭 집계.  
     - 컨테이너 재시작 실패와 같은 진단 정보 제공.

- **Key-Value 쌍 기반 시간 데이터베이스**를 사용하는 도구를 선택하면 메트릭 속성의 세부 추적이 가능해져 문제를 분석하고 해결하는 데 큰 도움을 줍니다.

---

Monitoring Kubernetes Using Prometheus

스킵

---

### **Kubernetes 로그 개요**

Kubernetes 환경을 완벽히 이해하려면 **메트릭 수집** 외에도 클러스터 및 배포된 애플리케이션에서 발생하는 **로그를 수집 및 중앙화**해야 합니다. 

로그 수집에서 흔히 할 수 있는 실수는 "모든 로그를 저장하자"는 접근 방식입니다. 그러나 이는 두 가지 문제를 초래할 수 있습니다:
1. **노이즈 증가**: 중요한 문제를 신속히 찾기 어려움.
2. **자원 소모**: 로그 저장 비용이 급증.

로그 수집과 저장은 환경에 따라 달라지며, 시간이 지남에 따라 환경에 필요한 정보를 명확히 이해하고 불필요한 노이즈를 제거해야 합니다. 또한, **보존 및 아카이빙 정책**을 설정하여 로그 관리를 효율화해야 합니다.  
- 일반적으로 **30~45일간의 로그 보존**이 적절합니다.  
- 규제나 준수 요구사항이 있는 경우에는 비용 효율적인 스토리지로 아카이빙을 고려합니다.

### **Kubernetes에서 수집해야 할 주요 로그**
Kubernetes 환경에서 다음 구성 요소의 로그를 수집하는 것이 중요합니다:

1. **노드 로그**  
   - 워커 노드에서 실행되는 Docker 데몬 로그.  
   - Docker 데몬의 상태는 컨테이너 실행에 필수적이며, 문제 해결에 필요한 정보를 제공합니다.  

2. **Kubernetes 컨트롤 플레인 로그**  
   - API 서버: `/var/log/kube-apiserver.log`  
   - 스케줄러: `/var/log/kube-scheduler.log`  
   - 컨트롤러 매니저: `/var/log/kube-controller-manager.log`  
     - 예: 서비스가 `LoadBalancer` 타입으로 생성된 후 **Pending 상태**에 멈춰 있을 경우, 컨트롤러 매니저 로그를 통해 근본적인 문제를 파악할 수 있습니다.  

3. **Kubernetes 감사(Audit) 로그**  
   - 시스템 내에서 "누가, 무엇을 했는지"를 추적할 수 있는 **보안 모니터링** 역할.  
   - 초기화 시 로그 스파이크가 발생할 수 있으므로 Kubernetes 문서를 참고해 로그를 튜닝하는 것이 중요.  

4. **애플리케이션 컨테이너 로그**  
   - **STDOUT로 로그 전송**: 표준 방식으로 중앙 집중형 로그 수집기가 Docker 데몬을 통해 로그를 직접 수집 가능.  
   - **사이드카 패턴**: 애플리케이션이 파일 시스템에 로그를 기록할 경우, 로그 전달을 위한 컨테이너를 Pod 내에 함께 실행.

---

### **Kubernetes 로그 수집 도구**

Kubernetes와 애플리케이션의 로그를 수집하기 위한 도구는 다양하며, 이미 사용 중인 도구를 활용하면 학습 곡선을 줄일 수 있음. 도구 선택 시 다음 조건을 고려해야 함:
1. Kubernetes **DaemonSet**으로 실행 가능.  
2. **STDOUT**을 사용하지 않는 애플리케이션을 위해 **사이드카 방식**도 지원.

#### **주요 로그 수집 도구**
1. **Elastic Stack (ELK)**  
   - **구성**: Elasticsearch, Logstash, Kibana로 이루어진 오픈 소스 솔루션.  
   - **특징**: 
     - 로그 집계, 처리, 검색, 시각화에 강력.  
     - Filebeat를 DaemonSet으로 배포하여 Kubernetes 로그를 수집 가능.  
   - **활용 예**: 클러스터와 애플리케이션 로그를 Kibana 대시보드에서 실시간 분석.

2. **Datadog**  
   - **종류**: SaaS 기반 통합 모니터링 플랫폼.  
   - **특징**: 
     - Kubernetes 환경에서 로그, 메트릭, 트레이싱을 통합적으로 지원.  
     - 로그 자동 태깅 기능으로 Kubernetes 리소스와 연결된 로그를 효율적으로 관리.  

3. **Sumo Logic**  
   - **종류**: SaaS 기반 로그 관리 및 분석 플랫폼.  
   - **특징**: 
     - Kubernetes 전용 통합 대시보드 제공.  
     - 로그와 메트릭을 통합하여 클러스터 상태를 종합적으로 분석 가능.  

4. **Sysdig**  
   - **종류**: Kubernetes 및 Docker 환경에 최적화된 상용 도구.  
   - **특징**: 
     - 보안 및 컴플라이언스 분석 기능 제공.  
     - 로그와 시스템 호출(trace) 데이터를 통합적으로 수집 및 분석.  

5. **클라우드 제공 서비스**
   - **Google Cloud Operations (구 Stackdriver)**  
     - **특징**: GKE 환경과 깊은 통합.  
     - 기본 제공되는 Kubernetes 로그 대시보드 활용 가능.  
   - **Azure Monitor for Containers**  
     - **특징**: AKS 환경의 컨테이너 로그 수집 및 분석 지원.  
     - Kubernetes Metrics API와 통합하여 CPU, 메모리 로그도 함께 수집.  
   - **Amazon CloudWatch**  
     - **특징**: EKS 및 ECS 환경에서 로그와 메트릭을 집계하여 모니터링.  
     - Container Insights를 통해 상세한 로그 분석 및 시각화 제공.  

#### **도구 선택 팁**
- 오픈 소스 솔루션을 선호한다면 **Elastic Stack**이 적합.  
- SaaS 기반의 간단한 설정과 높은 확장성을 원한다면 **Datadog**이나 **Sumo Logic**을 고려.  
- 클라우드 제공 환경(GKE, AKS, EKS)에서 운영 중이라면 클라우드 제공 도구를 활용하여 비용 및 통합성을 최적화.  
---
 Logging by Using an EFK Stack

---
### **Alerting 설계 시 핵심 요소**

1. **무엇을 경고해야 하는가?**
   - Kubernetes의 **자가 복구 기능**(예: Pod 자동 재시작)을 활용하여 단순 이벤트는 경고 대신 모니터링에 의존.  
   - **서비스 수준 목표(SLO)**를 기반으로 사용자 경험에 실질적으로 영향을 미치는 문제를 경고 대상으로 설정.  
     - 예: 프론트엔드 서비스 응답 시간이 SLO에서 정의한 20ms를 초과하는 경우 경고 트리거.

2. **경고가 필요한 시점**
   - 단순 리소스 초과(예: CPU, 메모리 사용량)보다 **즉각적인 인간 개입**이 필요한 상황에 초점.  
   - **자가 해결된 문제**에 대한 경고는 불필요한 알림일 가능성이 높음.

3. **자동화된 문제 해결**
   - 경고가 아닌 **자동 복구**로 대체 가능한 문제는 자동화:
     - 디스크 공간 부족 시 불필요한 로그 삭제.  
     - **Liveness Probe**를 이용해 비정상 프로세스 자동 재시작.

4. **경고 임계값 설정**
   - 너무 짧은 임계값은 **오탐(false positive)**을 유발.  
   - 기본적으로 **5분 이상의 임계값** 설정을 권장하며, 상황에 따라 10분, 30분, 1시간 등 표준화된 시간을 선택.  
   - 임계값 설정 시 환경에 적합한 실험을 통해 적절한 균형을 찾음.

5. **관련 정보 포함**
   - 경고 알림에 필요한 정보를 포함하여 신속한 대응 가능:
     - **문제 해결 플레이북 링크**.
     - 문제 발생 위치(데이터 센터, 지역), 앱 소유자, 영향을 받은 시스템.  
     - 발생 원인(예: 특정 서비스 또는 Pod).

6. **알림 채널 최적화**
   - **책임자가 확인 가능한 채널**로 알림 전달.  
   - 팀 이메일 또는 대규모 배포 그룹 사용 시 노이즈로 인해 무시될 가능성 증가.  
   - Slack, PagerDuty, OpsGenie 같은 실시간 협업 도구와 통합 권장.

7. **점진적 개선**
   - 완벽한 경고 체계는 처음부터 존재하지 않음.  
   - 초기 설정 후 모니터링 데이터를 분석하고, 불필요한 경고를 줄이면서 지속적으로 최적화:
     - 피로도를 낮추기 위해 **경고의 우선순위와 필요성**을 재검토.  
     - 운영 중의 피드백을 반영하여 대응 프로세스 개선.  

---

### **모니터링, 로깅, 알림에 대한 모범 사례**

#### **1. 모니터링**  
효율적인 모니터링은 시스템 상태를 실시간으로 파악하고 문제를 사전에 감지하는 데 핵심적인 역할을 합니다.  

- **노드 및 Kubernetes 구성 요소 모니터링**: CPU, 메모리, 네트워크, 디스크 사용량과 같은 리소스 사용량뿐만 아니라 오류율과 포화 상태를 추적.  
- **블랙박스 모니터링**: 시스템의 외부 동작과 사용자 경험을 기반으로 한 증상 중심의 모니터링.  
  - 예: 서비스 응답 시간, HTTP 상태 코드 비율 등.  
- **화이트박스 모니터링**: 시스템 내부 상태를 계측하여 원인 분석에 도움.  
  - 예: 컨테이너별 메모리 사용량, API 호출 지연 시간.  
- **고정밀 데이터 수집**: Prometheus 등으로 시간 시계열 데이터를 수집하고, 정확한 상태를 시각화.  
- **메트릭 유형 활용**:  
  - **평균 메트릭**: 특정 자원 집합의 대표적인 성능을 시각화.  
  - **합계 메트릭**: 데이터를 집계해 전체 시스템 상태를 분석.  

#### **2. 로깅**  
로그는 문제 발생 시 상세한 원인 분석을 가능하게 하며, 모니터링과 상호보완적으로 작동합니다.  

- **모니터링과 로그 통합**: 메트릭과 로그를 함께 활용하여 전체 시스템 상태를 포괄적으로 분석.  
- **효율적인 보존 정책**:  
  - 일반적으로 **30~45일간 보존**.  
  - 장기 보관은 저비용 스토리지로 이동.  
- **로그 수집 방법**:  
  - **STDOUT 기반 전달**: DaemonSet을 통해 컨테이너 로그를 수집 및 전달.  
  - **사이드카 패턴 제한 사용**: 파일 기반 로그의 수집은 필요 시에만 적용, 리소스 오버헤드를 고려.  


#### **3. 알림**  
적절한 알림 설계는 대응 시간을 단축하고, 경고 피로를 줄여 효율성을 높입니다.  

- **증상 중심의 경고 설정**:  
  - 단순 리소스 초과보다는 사용자 경험(SLO)에 영향을 미치는 문제를 우선 경고.  
  - 예: 서비스 응답 시간 초과, HTTP 오류 비율 증가.  
- **자동화 활용**: 경미한 문제는 자동 복구로 처리하여 경고를 줄임.  
  - 예: 디스크 포화 시 자동 로그 삭제.  
- **경고의 맥락 제공**: 알림에 문제 해결 정보를 포함하여 신속한 대응 가능.  
  - 예: 문제 원인, 영향을 받은 시스템, 문제 해결 플레이북 링크.  
- **책임자 대상 알림**: 직접 관련된 엔지니어와 팀에 알림 전달.  
  - Slack, PagerDuty, OpsGenie 등의 실시간 도구 활용.  

**핵심 요약**  
- **모니터링**: 사용자 경험 중심의 증상과 시스템 내부 상태를 조화롭게 계측.  
- **로깅**: 단기 보존과 장기 저장을 조합하고, 로그와 메트릭을 통합 분석.  
- **알림**: 경고를 줄이고 자동화를 우선 적용하며, 중요한 문제는 맥락과 함께 전달.  

이러한 모범 사례는 Kubernetes 환경의 신뢰성을 높이고, 효율적인 운영을 지원합니다.

