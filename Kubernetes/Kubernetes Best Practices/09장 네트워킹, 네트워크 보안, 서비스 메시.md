# 9.1 쿠버네티스 네트워크 원칙

### **동일한 파드 내의 컨테이너 간 통신**

- 동일한 파드 내의 모든 컨테이너는 동일한 네트워크 네임스페이스 공유
- localhost 통신 가능
- 포트 충돌 방지를 위해 다른 포트 사용함
- 파드의 네트워킹을 담당하는 모든 파드에서 일시 중지된 컨테이너를 사용해 파드 내 모든 컨테이너가 동일한 로컬 네트워크에 존재하게 함

### **파드 간의 통신**

- NAT(네트워크 주소 변환) 없이 통신함
- 동일한 노드에 존재하는 파드 사이에서 이 규칙 적용
- 동일한 클러스터의 서로 다른 노드에 존재하는 파드 사이에서도 적용

### 서비스와 파드 간의 통신

- 서비스는 고정된 IP 주소와 포트를 나타냄
- 각 노드는 서비스에 연계된 엔드포인트로 트래픽 전달
- 서비스를 구현하는 기술
    1. iptables 이용 ⇒ 현재는 대부분 1번 사용
    2. IP 가상 서버 사용
- iptables는 의사계층 로드 밸런서를 제공
    - iptables는 네트워크 주소 변환(NAT)을 수행해 서비스와 파드 간 트래픽을 로드 밸런싱함.

# 9.2 네트워크 플러그인

:  플러그인 아키텍처를 기반으로 한 네트워킹 표준이 도입되어 여러 서드파티 네트워킹 프로젝트와 연동 가능

## 9.2.1 Kubenet

- 쿠버네티스의 기본 네트워크 플러그인
- 가상 이더넷 쌍과 리눅스 브릿지(cbr0)를 제공
- IP 할당으로 CIDR(사이더, classless inter-domain routing) 범위 내 IP 주소 받음
- IP 위장 옵션으로 파드 CIDR 범위 외부로 트래픽을 보낼때 NAT를 사용하며, IT 위장 플래그를 설정함

## 9.2.2 Kubenet의 모범 사례

### **단순한 네트워크 스택**

- Kubenet으로 단순한 네트워크 스택 구축
- 복잡한 네트워크 환경에서 IP 주소 절약
- ex) 온프레미스 데이터센터에 확장된 클라우드 네트워크

### **CIDR 범위 설정**

- 클러스터의 잠재적 최대 파드 수를 처리할 수 있도록 충분히 큰 CIDR 범위를 설정

### **계획 설정**

- 적합한 노드로 트래픽이 전송되도록 경로 규칙을 이해하고 올바른 계획 설정
- 일반적으로 클라우드 공급자 환경에서는 자동화되나, 온프레미스 또는 예외적인 경우에 관리 필요

## 9.2.3 CNI 플러그인

CNI(Container network interface) 명세를 준수하는 플러그인

### **CNI 플러그인 역할**

- CNI와의 인터페이스, 기본 API 동작, 컨테이너 런타임과의 인터페이스
- **필수 기능**: IP 주소 관리, 네트워크 컨테이너 추가 및 삭제 지원
- rkt 네트워킹 제안서에서 파생

### **CNI 플러그인 예시**

- **클라우드 제공**: Azure Native CNI, Amazon VPC CNI
- **전통적 네트워크 솔루션**: Nuage Networks, Juniper Contrail/Tungsten Fabric, VMware NSX

## 9.2.4 CNI 플러그인 모범 사레

- 애플리케이션 통신의 신뢰성을 위해 가상 및 물리적 네트워크 설계를 신중히 설계해야함
    
    **CNI 플러그인 선택**
    
    - 고가용성, 다중 클라우드 연결성, 네트워크 정책 지원 등 기능을 평가
    - 클라우드 제공자의 SDN이 CNI 플러그인을 지원하는지 확인
    
    **도구 호환성**
    
    - 보안, 관찰성, 관리 도구가 CNI와 호환되는지 검토
    - 예: **Weave Scope**, **Dynatrace**, **Sysdig** 등
    
    **클라우드 네이티브 도구 활용**
    
    - Azure AKS, Google GCE, Amazon EKS 환경에서 **Azure Container Insight**, **Stackdriver**, **CloudWatch** 같은 내장 도구 활용
    - 네트워크 스택과 네 가지 황금 신호(레이턴시, 트래픽, 오류, 포화도)를 모니터링
    
    **주소 공간 및 오버레이 네트워크**
    
    - SDN(Software Defined Networking) 없이 CNI를 사용하는 경우, 노드 IP, 파드 IP, 내부 로드 밸런서 확장 시 오버헤드를 감당할 충분한 네트워크 주소 공간 확보 필요

## 9.3 쿠버네티스의 서비스

### **필요성**

- 쿠버네티스 기본 네트워크 규칙과 CNI 플러그인을 인해 오직 같은 클러스터 내의 다른 파드와 직접 통신
- 일부 CNI 플러그인은 클러스터 외부에서 파드 IP를 통해 직접 접근 가능하지만, 파드의 짧은 수명으로 인해 비효율적
- 파드가 중단되거나 재생성될 때 이름과 IP가 변경되므로, 이를 해결하기 위해 서비스 API가 필요

### **구현 방법**

- 내구성 있는 IP와 포트를 쿠버네티스 클러스터 내에 할당
- 서비스 API는 서비스의 엔드포인트를 적절한 파드에 자동으로 매핑
- `kube-proxy` 서비스가 각 노드에서 실행되며, **iptables** 규칙을 사용해 서비스 IP/포트를 파드의 실제 IP로 매핑
- 서비스 타입은 엔드포인트 노출 범위(내부/외부)를 결정하며, 다음 절 네 가지 기본 서비스 타입이 제공됨

## 9.3.1 ClusterIP 서비스 타입

- 기본 값은 ClusterIP, 지정된 서비스 CIDR 범위 내에 IP가 할당됨을 의미
- 이 IP는 서비스 객체와 오래 지속되므로 셀렉터 필드를 통해 백엔드 파드에 IP, 포트, 프로토콜을 매핑(**셀렉터 필드가 없는 경우 아래에 설명**)
- 서비스를 선언해 서비스 DNS 이름을 만들고, 이를 통해 클러스터 내에서 서비스 검색을 용이하게 함
- 워크로드는 서비스 이름으로 DNS를 조회해 클러스터 내의 다른 서비스와 통신

<aside>
💡

다른 네임스페이스 서비스를 찾을 때 사용하는 DNS 패턴은 <service_name〉.<namespace_name>. svc. cluster, local

</aside>

### **셀렉터 없는 서비스**

- **엔드포인트 API**를 사용해 명시적으로 IP와 포트를 정의 가능
- 셀렉터로 자동 업데이트 대신 특정 엔드포인트를 수동으로 추가
- 예시 시나리오 : 테스트 단계에서 클러스터에 없는 데이터베이스를 사용하고 나중에 쿠버네티스에 배포되는 데이터베이스로 서비스를 변경

### **헤드리스 서비스**

- `kube-proxy`가 관리하지 않는 서비스
- 직접 엔드포인트 관리 가능
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/bf8b5dbc-826d-4e7f-8b31-80262575587f/a4cb114e-7fc7-4c6c-bc01-83065864d07a/image.png)
    

## 9.3.3 ExternelName 서비스 타입

- 현업에서는 거의 쓰이지 않으나, 클러스터 수준의 내구성을 가진 DNS 이름을 외부 DNS 서비스에 전달할 때 유용
- ex) 클라우드 제공자의 외부 데이터베이스 서비스 (`mymongodb.documents.azure.com`)
- **Environment 변수** 대신 사용 시 유리, 서비스 명세만 수정해 파드 재시작 없음

## 9.3.4 로드 밸런서 서비스 타입

- 클라우드 공급자 기반 로드 밸런싱 메커니즘을 배포하는 유일한 방법
- 클라우드 인프라에서 로드 밸런서 생성을 자동화

### 동작 방식

- 대부분의 클라우드 공급자(AWS, Azure, GCE, OpenStack)에서 유사한 방식으로 동작
- 기본적으로 **공개 로드 밸런서**를 생성
- **구성 옵션**:
    - 클라우드 공급자의 내부 전용 로드 밸런서 사용 가능
    - AWS의 일래스틱 로드 밸런싱 등 특정 설정을 활성화하는 **애너테이션** 지원
    - 서비스 명세에서 로드 밸런서의 IP 및 허용 소스 범위를 정의

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/bf8b5dbc-826d-4e7f-8b31-80262575587f/95904a92-87ed-483c-be60-a25d874c6092/image.png)

## 9.3.5 인그레스와 인그레스 컨트롤러

**:**  쿠버네티스 워크로드의 HTTP 트래픽을 효율적으로 관리하며, 고급 애플리케이션 제어를 위한 필수 도구

- 쿠버네티스 서비스 타입은 아니지만, 워크로드의 **트래픽 관리에** 핵심 기능
- 서비스 API가 제공하는 L3/L4 로드 밸런싱을 넘어 애플리케이션 레벨(L7)의 고급 트래픽 제어 필요

**인그레스 API**

- HTTP 라우팅, 호스트 및 경로 기반 규칙으로 특정 서비스에 트래픽 전달
- **호스트 기반 라우팅**: 단일 인그레스에서 여러 호스트 관리 가능
- TLS 지원 및 404 오류 대체 응답 제공
- 이를 활용하면 사용자 경험(UX) 개선과 트래픽 관리 최적화를 동시에 달성할 수 있음

## 9.3.6 서비스와 인그레스 컨트롤러 모범 사례

- 서비스와 인그레스 컨트롤러를 신중하게 설계하고 표준화 및 성능 관리로 복잡한 네트워크 환경을 효과적으로 관리
    1. **서비스 노출 제한**
        - 클러스터 외부에 노출하는 서비스는 최소화
        - 대부분의 서비스는 **ClusterIP**로 설정하고, 외부에서 접근해야 하는 서비스만 공개
    2. **인그레스 API와 컨트롤러 활용**
        - HTTP/HTTPS 트래픽은 **인그레스 API**를 통해 라우팅
        - **TLS 종료**, **속도 제한**, **헤더 재작성**, **OAuth 인증** 등의 기능을 애플리케이션에 구현하지 않고 인그레스 컨트롤러로 처리
    3. **컨트롤러 표준화**
        - 특정 인그레스 컨트롤러를 조직적으로 표준화하여 사용
        - 설정 차이를 줄여 쿠버네티스 배포의 이식성을 향상
    4. **클라우드 서비스 평가**
        - 클라우드 서비스 공급자가 제공하는 **인그레스 컨트롤러**의 기능 평가.
        - 쿠버네티스 API와의 통합 가능 여부 확인.
    5. **API 기반 워크로드 관리**
        - API 조율이 필요한 경우 **Kong** 또는 **Ambassador** 같은 컨트롤러 평가.
        - **NGINX**, **Traefik**도 사용 가능하지만 API 프록시 기능은 제한적.
    6. **고가용성과 성능 모니터링**
        - 인그레스 컨트롤러를 **고가용성**으로 설계.
        - 메트릭 기반으로 성능을 모니터링하고 필요에 따라 적절히 확장.
        - 워크로드 확장 시에도 클라이언트 중단을 방지할 대책 마련.

# 9.4 네트워크 보안 정책

- 네트워크 폴리시는 쿠버네티스 클러스터 내 트래픽 제어를 위한 도구
- 기본적으로 화이트리스트 접근 방식을 채택해 필요 없는 네트워크 접근 제한
- 따라서 애플리케이션 계층 간 통신을 보장하고 불필요한 트래픽 차단함
    
    ### **네트워크 폴리시의 역할**
    
    - 네트워크폴리시 API를 통해 **인그레스(수신)** 및 **이그레스(송신)** 트래픽을 제어
    - 파드 그룹 간 또는 외부 엔드포인트와의 통신을 제한
    - 네트워크 정책은 지원되는 네트워크 플러그인과 함께 사용해야 함
    
    ### **구성 요소**
    
    - **`podSelector`**: 필수 필드이며, 쿠버네티스의 `matchLabels` 셀렉터 규칙을 따름
    - **`ingress`/`egress`**: 소스나 대상 규칙 목록(CIDR 범위, podSelector, namespaceSelector).
        - 비워두면 해당 방향의 모든 트래픽 차단
    - **`policyTypes`**: 정책이 적용되는 트래픽 타입을 명시
        - `ingress`는 명시적 정의 없이 동작
        - `egress`는 반드시 `policyTypes`와 함께 정의되어야 적용
    
    ### **정책 적용 범위 및 작동 방식**
    
    - 네임스페이스 단위 객체로, 특정 네임스페이스의 모든 파드에 적용 가능
    - 셀렉터와 일치하지 않는 파드에는 정책이 적용되지 않으며, 기본적으로 모든 트래픽이 허용
    - 셀렉터와 일치하는 파드에는 정의된 인그레스/이그레스 규칙 외의 트래픽 차단(화이트리스트 방식)
    
    ### **추가 제어 옵션**
    
    - 특정 **포트 및 프로토콜**로 통신 범위를 세분화 가능.
    - 여러 네트워크폴리시를 중첩 적용 가능하며, 규칙은 상호 보완적
    
    ### 사용예시 : **애플리케이션 계층 제한**
    
    - 레이블: `tier: "web"`, `tier: "api"`, `tier: "db"`로 구분된 세 계층
    - 네트워크폴리시를 통해 각 계층 간 트래픽을 제한하며, 기본적으로 외부 트래픽은 거부

## 9.4.1 네트워크 정책 모범 사례

- 네트워크 정책을 점진적으로 적용하여 예상치 못한 차단이나 허용을 방지.
- 명확한 구조와 레이블로 복잡한 네트워크 설정도 쉽게 관리 가능
- 테스트 환경에서 검증 후 실 환경에 안전하게 배포.
    - 전통적인 네트워크 보안 방식과 달리, **쿠버네티스 네트워크 정책**은 애플리케이션 트래픽을 중심으로 관리
    - 물리적 장치 대신 소프트웨어 정의 방식을 통해 더 유연하고 애플리케이션 친화적
    - **점진적 적용**:
        - 먼저 **인그레스 트래픽**에 집중하고, 예상대로 흐름이 동작하면 **이그레스 규칙**을 추가
        - 복잡한 규칙은 네트워크 디버깅을 어렵게 하므로 간단하게 시작
    - **플러그인 호환성 확인**:
        - 사용 중인 네트워크 플러그인이 `네트워크폴리시 API`를 지원하는지 확인.
        - 예: Calico, Cilium, Romana, Weave Net 등
    - **기본 거부 정책 설정**:
        - 클러스터의 각 네임스페이스에 기본 거부(NetworkPolicy) 설정을 추가해, 실수로 모든 트래픽이 허용되지 않도록 방지
        - 네트워크 정책 삭제 시에도 안전한 기본값 유지
    
    ### **구체적인 설정 사례**
    
    - **외부 접근 허용 파드**:
        - 인터넷 접근이 필요한 파드에 레이블(e.g., `allow-internet=true`)을 사용해 명시적 수신 허용
        - 로드 밸런서, 방화벽 등 네트워크 장치의 흐름을 파악해 정책 설계
    - **애플리케이션 분리**:
        - 애플리케이션 워크로드를 단일 네임스페이스에 배치하여 정책 관리를 단순화
        - 네임스페이스 간 통신 시 명확한 이름과 레이블 사용
    - **테스트 네임스페이스 활용**:
        - 테스트 전용 네임스페이스를 만들어 제한된 정책으로 트래픽 패턴과 동작을 검증

# 9.5 서비스 메시

- 서비스 메시를 통해 복잡한 서비스 간 통신을 효과적으로 관리 가능
- 각 도구는 고유한 기능 제공 및 애플리케이션의 복잡도를 줄이면서 보안과 안정성을 강화
- SMI**(Service Mesh Interface)**와 같은 표준화된 인터페이스는 다양한 메시 도구와 호환성 제공
    - **정의**: 수많은 서비스와 엔드포인트가 상호 통신하며, 보안, 관찰, 추적 등을 관리하기 위한 구조
    - **구성 요소**:
        - **데이터 플레인**: 실제 트래픽 제어 및 처
        - **컨트롤 플레인**: 정책 설정, 보안, 트래픽 제어 동기화.
    - **목적**: 애플리케이션의 변경 없이 트래픽 제어, 보안, 로드 밸런싱, 장애 복원 등을 제공
    
    ### **주요 기능**
    
    - **트래픽 제어**: 세밀한 로드 밸런싱 및 트래픽 정책 관리
    - **서비스 디스커버리**: 클러스터 내부 및 외부 서비스의 자동 탐색
    - **관찰 가능성**: 추적 시스템(예: Jaeger, Zipkin)을 통한 분산 서비스 추적 및 모니터링
    - **보안 강화**: 파드 간 및 데이터 센터 간 트래픽 상호 인증과 암호화 제공
    - **복원력 및 장애 방지**: 재시도, 데드라인 설정 등 네트워크 장애에 대한 대응
    
    ### **서비스 메시 도구**
    
    - **Istio**:
        - 구글, Lyft, IBM이 공동 개발
        - 데이터 플레인으로 Envoy 프록시 사용
        - Mixer, Pilot, Galley, Citadel 등 컨트롤 플레인 컴포넌트 제공.
    - **Linkerd 2**:
        - Rust로 작성된 자체 데이터 플레인 프록시 사용.
        - 간단하고 경량화된 서비스 메시 기능 제
    - **Consul**:
        - HashiCorp 제공
        - 자체 프록시 또는 Envoy 선택 가능
        - 쿠버네티스 통합 및 지원 제공
    
    ### **SMI (Service Mesh Interface)**
    
    - 마이크로소프트, Linkerd, HashiCorp 등에서 개발한 표준 인터페이스.
    - 기능:
        - 트래픽 원격 분석 및 관리
        - 서비스 간 암호화 및 인
        - 트래픽 정책과 메트릭 수집.
    - **목표**: 다양한 서비스 메시 도구 간 호환성과 차별화를 동시에 지원

## 9.5.1 서비스 메시 모범 사례

- 필요한 기능과 오버헤드 고려
- 다중 클라우드 및 하이브리드 환경 고려
- 상업적 지원 여부 평가
    1. **필요한 기능과 오버헤드 고려**
        - 서비스 메시가 제공하는 주요 기능(트래픽 제어, 보안, 관찰 등)의 **중요도를 평가**하여 선택
        - **오버헤드 최소화**: 인적 기술 부채(학습 곡선, 관리 복잡성)와 인프라 자원 부채(리소스 소모)를 고려
        - TLS 통합 CNI 사용: 특정 파드 간 보안 통신이 필수적이라면 CNI에 TLS가 포함된 솔루션이 적합
    2. **다중 클라우드 및 하이브리드 환경 고려**
        - 다중 클라우드나 하이브리드 환경 간 메시가 필요한 경우, 해당 기능을 **지원하는 서비스 메시**를 선택
        - 이러한 시나리오에서는 메시가 복잡하고 불안정해질 가능성을 염두에 두어야 함
    3. **상업적 지원 여부 평가**
        - 오픈 소스 기반 서비스 메시의 경우, 관리 팀이 기술에 익숙하지 않다면 **상업적 지원 제품**이 유리
        - 예: **Istio**는 강력하지만 복잡하기 때문에 상업적 지원을 통해 관리 효율성을 높일 수 있음

# 9.6 마치며

…